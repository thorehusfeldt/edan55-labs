\documentclass{tufte-handout}
\usepackage{amsmath,amsthm}

\input{vc.tex}

\usepackage{pgfplots}
\pgfplotsset{width=\textwidth,compat=1.5.1}

\newtheorem{claim}{Claim}[section]
\title{\sf Pagerank}
%\date{\GITAuthorDate}
%\author{Thore Husfeldt}

\begin{document}
\maketitle
\footnotetext{rev. \GITAbrHash}

\section{Pagerank}

\begin{marginfigure}
\begin{tikzpicture}
\node (0) [draw,circle] at (0,0) {0};
\node (1) [draw,circle] at (1,1) {1};
\node (2) [draw,circle] at (2,-.5) {2};
\node (3) [draw,circle] at (3,.5) {3};
\draw [->] (0) to [bend left] (1);
\draw [->] (0) to  (1);
\draw [->] (0) to [bend right] (2);
\draw [->] (1) to  (2);
\draw [->] (2) to  (0);
\draw [->] (2) to [bend right] (1);
\draw [->] (2) to (3);
\end{tikzpicture}
\caption{A directed multigraph.}
\end{marginfigure}
Consider an finite, directed multigraph $G=(V,E)$ without self-loops, as in
Fig.~1.
We will understand $G$ as the hyperlink structure of the web pages
described by $V$; an arc from vertex $u$ to vertex $v$ describes a
hyperlink from page $u$ to page $v$.

The \emph{random surfer} model is a stochastic process that aims to
rank the relevance of a these pages.
The states of this process are the vertices $V$.
With good probability $\alpha$, the process picks an outgoing edge at
random and moves to that page.
If there are no outgoing edges, the surfer picks a random page from
$V$ instead.
(Note that outgoing edges are counted with multiplicities, so from
vertex~0 in the example, the chance of going to 1 is twice that of
going to 2.)
Alternatively, with probability $(1-\alpha)$, the surfer becomes bored
and moves to a random page from $V$ instead.
The probability $\alpha$ is called the \emph{damping factor}; a
typically value that works well for web pages is $\alpha = \frac{85}{100}$. 

This process is a finite, irreducible, and ergodic Markov chain.
Its stationary distribution describes the \emph{page rank} of each
vertex. The contribution of Serge Brin and Larry Page was to
realise that this value gives a good measure of the relevance of a web
page, which is the main idea behind the search engine Google.

\subsection{Files}


\begin{marginfigure}
\begin{verbatim}
4
0 1   0 1   0 2
1 2   
2 0   2 1
2 3
\end{verbatim}
\caption{Input file for the graph in Fig.~1.}
\end{marginfigure}

Vertex names are integers $V= \{0,\ldots, n-1\}$.
Input files contain $|V|$, followed by $u$ and $v$ for each $(u,v)\in
E$.
The files are in the data directory are:
\begin{quotation}
\begin{description}
\item[three.txt] The 4-vertex graph from Fig.~1.
\item[tiny.txt] The 5-vertex graph from \S{}1.6. from Sedgewick and
  Wayne\sidenote{R. Sedgewick and K. Wayne,
  \emph{Programming in Java: An Interdisciplinary Approach}, Addison
  Wesley, 2007.}
\item[medium.txt] The 50-vertex graph \emph{ibid.}
\item[wikipedia.txt] The 11-vertex graph from Wikipedia's PageRank article
  \sidenote{``PageRank.'' Wikipedia, The Free Encyclopedia. Wikimedia
  Foundation, Inc. Accessed 16 Sep 2012.}
\item[p2p-Gnutella08-mod.txt] A 6301-vertex graph describing a file
  sharing network. 
Vertices represent hosts in the Gnutella network topology and edges
represent connections between the Gnutella host, collected 8 August
2002.\sidenote{Modified from the Stanford University SNAP library,
  original file at
  snap.stanford.edu/data/p2p-Gnutella08.html. Sources:
J. Leskovec, J. Kleinberg and C. Faloutsos. \emph{Graph Evolution: Densification and Shrinking Diameters}. ACM Transactions on Knowledge Discovery from Data (ACM TKDD), 1(1), 2007.
M. Ripeanu and I. Foster and A. Iamnitchi. \emph{Mapping the Gnutella
  Network: Properties of Large-Scale Peer-to-Peer Systems and
  Implications for System Design}. IEEE Internet Computing Journal,
2002.}
\end{description}
\end{quotation}

\subsection{Deliverables}

\begin{enumerate}
\item Implement a simulation of the random surfer model. 
  Start in
  vertex 0 and follow the rules for a given number of iterations read
  from the command line. Count the number of times each vertex is
  visited and print the relative frequencies.
\item Solve the exact same problem using linear algebra instead of
  simulation.
  That is, construct the transition probability matrix $P$ and
  compute $pP^r$ for some sufficiently high $r$ (read from the command
  line) and some initial vector $p$ of your choice.
  There are (at least) 3 ways of computing $pP^r$:
  \begin{enumerate} 
  \item Let $p_0= p$. For each $i=1,\ldots, m$ let $p_i = p_{i-1}
    P$. Return $p_r$.
  \item Compute $Q = P^r = P\cdot P \cdots P$ using $r-1$ matrix products.
    Return $pQ$.
  \item Compute $P^r$ by iterated squaring. Assume $r$ is a power of
    2. Set $Q_0 = P$ and for $i=1,\ldots,\log r$ compute $Q_i =
    Q_{i-1}^2$. Return $pQ_{\log r}$. 
  \end{enumerate}
  Pick the one you think is fastest or easiest to implement for the
  small instances.

  However, to attack an instance of nontrivial size, you need to
  exploit the structure of the transition matrix.
  (Unless you like waiting a lot.)
  Let $A$ denote the adjacency matrix of $G$.
  Then we have the hyperlink matrix $H$ defined as $H_{ij} = A_{ij} /
  \deg(i)$.
  Let $D$ denote the matrix where $D_{ij} = 0$ if $\deg(i) > 0 $ and
  $D_{ij} = 1/n$ if $\deg(i) = 0$ then
  \[ P = \alpha(H + D) + \frac{1-\alpha}{n} \bf 1\,,\]
  where $\bf 1$ is the $|V|\times |V|$ all-1s matrix.
  In particular,
  \[pP = \alpha p H + \alpha p D + \frac{1-\alpha}{n} p \bf 1\,.\] All
  three of these vector--matrix products are simpler to compute: $D$
  and $\bf 1$ have identical columns, and $H$ is sparse (so you don't
  store is as a square matrix of size $O(|V|^2)$; just as a list of
  $O(|E|)$ nonzero values.).
\item Fill out the report.
\end{enumerate}


\newpage
\section{Pagerank Lab Report}


by Alice Cooper and Bob Marley\sidenote{Complete the report by filling
  in your names and the parts marked $[\ldots]$.
  Remove the sidenotes in your final hand-in.}

\subsection{Transition probabilities}

The transition matrix for the graph described in three.txt
is\sidenote{Fill in the right values. Set $\alpha=\frac{85}{100}$.}
\begin{equation*}
P = 
\left(
\begin{array}{ccc}
1 & 6 & \pi \\
1 & 1/e & -2 \\
1 & 1 & 0 \\
\end{array}
\right)\,,
\end{equation*}
and its 10th power is
\begin{equation*}
P^{10} = 
\left(
\begin{array}{ccc}
1 & 6 & \pi \\
1 & 1/e & -2 \\
1 & 1 & 0 \\
\end{array}
\right)\,.
\end{equation*}

The transition matrix $P$ can be broken down into  $P = \alpha(H + D)
+ \frac{1-\alpha}{n} \bf 1$, where $H =[\ldots]$ and $D =[\ldots]$.

\subsection{Results}

The following table gives the top hits, i.e., the 10 first vertices of
each graph sorted by page rank, using $\alpha = \frac{85}{100}$.

\medskip
\begin{fullwidth}
\small
\begin{tabular}{lcccccccccc}
three.txt & 2 (36.6\%) & 1 (27.5\%) & 0 (18.4\%) & 3 (17.3\%) \\
tiny.txt & [\ldots] &\\
medium.txt &\\
wikipedia.txt & \\
p2p-Gnutella08-mod.txt &
\end{tabular}
\end{fullwidth}

\bigskip The following table gives the number of random walk steps and
(scalar) multiplications needed for each graph until the results were
stable to within 2 decimal places.

\medskip
\begin{fullwidth}
\small
\begin{tabular}{lcccccccccc}
Graph & \# transitions  & \# multiplications \\
three.txt & 54,325 \\
tiny.txt &\\
medium.txt &\\
wikipedia.txt & \\
p2p-Gnutella08-mod.txt
\end{tabular}
\end{fullwidth}

\subsection{Optional}

Build a time machine, fly back to the early 1990s.
Start a search engine company based on this idea.


\newpage
\section{Perspective}

The biggest thing missing from this lab is in the linear algebra part:
Transition matrices for web pages are very, very large and very, very
sparse.
So it makes sense to use a matrix multiplication algorithm for sparse
matrices.
In fact, for large datasets such as the world wide web, this is
crucial.




\end{document}
